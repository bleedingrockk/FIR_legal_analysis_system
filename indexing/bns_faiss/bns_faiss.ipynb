{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bed0dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4176704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # loads .env file\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"openai_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda2420",
   "metadata": {},
   "source": [
    "### Reading the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1933ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # pip install pymupdf\n",
    "\n",
    "def extract_with_headings(pdf_path, output_file=None):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    output_lines = []\n",
    "\n",
    "    for page_num, page in enumerate(doc, start=1):\n",
    "        output_lines.append(f\"\\n--- Page {page_num} ---\\n\")\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "\n",
    "        for b in blocks:\n",
    "            if \"lines\" not in b:\n",
    "                continue\n",
    "            for line in b[\"lines\"]:\n",
    "                for span in line[\"spans\"]:\n",
    "                    text = span[\"text\"].strip()\n",
    "                    size = span[\"size\"]\n",
    "                    font = span[\"font\"]\n",
    "\n",
    "                    # Simple rule: bigger font = heading\n",
    "                    if size >= 16:\n",
    "                        output_lines.append(f\"\\n[HEADING] {text}\\n\")\n",
    "                    else:\n",
    "                        output_lines.append(f\"{text} \")\n",
    "\n",
    "        output_lines.append(\"\\n\")\n",
    "    \n",
    "    doc.close()\n",
    "    \n",
    "    output_text = \"\".join(output_lines)\n",
    "    \n",
    "    if output_file:\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(output_text)\n",
    "        print(f\"Output saved to {output_file}\")\n",
    "    else:\n",
    "        print(output_text)\n",
    "    \n",
    "    return output_text\n",
    "\n",
    "extract_with_headings(\"THE_BHARATIYA_NYAYA_SANHITA_2023.pdf\", \"extracted_text.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3685f",
   "metadata": {},
   "source": [
    "### Regex "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d033f",
   "metadata": {},
   "source": [
    "#### Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3cad8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER I: PRELIMINARY\n",
      "CHAPTER II: OF PUNISHMENTS\n",
      "CHAPTER III: GENERAL EXCEPTIONS\n",
      "CHAPTER VII: OF OFFENCES AGAINST THE STATE\n",
      "CHAPTER VIII: OF OFFENCES RELATING TO THE ARMY,N AVY AND AIR FORCE\n",
      "CHAPTER IX: OF OFFENCES RELATING TO ELECTIONS\n",
      "CHAPTER XI: OF OFFENCES AGAINST THE PUBLIC TRANQUILLITY\n",
      "CHAPTER XII: OF OFFENCES BY OR RELATING TO PUBLIC SERVANTS\n",
      "CHAPTER XIII: OF CONTEMPTS OF THE LAWFUL AUTHORITY OF PUBLIC SERVANTS\n",
      "CHAPTER XIV: OF FALSE EVIDENCE AND OFFENCES AGAINST PUBLIC JUSTICE\n",
      "CHAPTER XX: REPEAL AND SAVINGS\n",
      "\n",
      "Total chapters found: 11\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Read the extracted text\n",
    "with open('extracted_text.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Regex to find chapters and clean headings\n",
    "pattern = r'CHAPTER([IVX]+)\\s+([A-Z\\s,]+?)(?=\\s+\\d+\\.|$)'\n",
    "chapters = []\n",
    "\n",
    "for match in re.finditer(pattern, text):\n",
    "    chapter_num = match.group(1)\n",
    "    heading = match.group(2).strip().lstrip()\n",
    "    \n",
    "    # Join single letters with next capitalized part: \"P RELIMINARY\" -> \"PRELIMINARY\"\n",
    "    parts = heading.split()\n",
    "    cleaned = []\n",
    "    i = 0\n",
    "    while i < len(parts):\n",
    "        if len(parts[i]) == 1 and i + 1 < len(parts) and parts[i+1][0].isupper():\n",
    "            cleaned.append(parts[i] + parts[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            cleaned.append(parts[i])\n",
    "            i += 1\n",
    "    \n",
    "    heading_clean = ' '.join(cleaned).replace(' ,', ',')\n",
    "    chapters.append({'chapter': f'CHAPTER {chapter_num}', 'heading': heading_clean})\n",
    "    print(f\"{chapters[-1]['chapter']}: {chapters[-1]['heading']}\")\n",
    "\n",
    "print(f\"\\nTotal chapters found: {len(chapters)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32778985",
   "metadata": {},
   "source": [
    "#### Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4014d861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. | 2. | 3. | 4. | 5. | 6. | 7. | 8. | 9. | 10. | 11. | 12. | 13. | 14. | 15. | 16. | 17. | 18. | 19. | 20. | 21. | 22. | 23. | 24. | 25. | 26. | 27. | 28. | 29. | 30. | 31. | 32. | 33. | 34. | 35. | 36. | 37. | 38. | 39. | 40. | 41. | 42. | 43. | 44. | 45. | 46. | 47. | 48. | 49. | 50. | 51. | 52. | 53. | 54. | 55. | 56. | 57. | 58. | 59. | 60. | 61. | 62. | 63. | 64. | 65. | 66. | 67. | 63. | 68. | 63. | 64. | 69. | 70. | 71. | 72. | 73. | 74. | 75. | 76. | 77. | 78. | 79. | 80. | 81. | 82. | 83. | 84. | 85. | 86. | 87. | 88. | 89. | 90. | 91. | 92. | 93. | 94. | 95. | 96. | 97. | 98. | 99. | 98. | 100. | 101. | 102. | 103. | 104. | 105. | 106. | 107. | 108. | 109. | 110. | 111. | 112. | 113. | 114. | 115. | 116. | 117. | 118. | 119. | 120. | 121. | 122. | 101. | 123. | 124. | 125. | 126. | 127. | 128. | 129. | 130. | 131. | 132. | 133. | 134. | 135. | 136. | 131. | 137. | 138. | 139. | 140. | 141. | 142. | 143. | 145. | 146. | 147. | 148. | 149. | 150. | 151. | 152. | 153. | 147. | 154. | 155. | 156. | 157. | 158. | 159. | 160. | 161. | 154. | 162. | 163. | 164. | 165. | 166. | 167. | 168. | 169. | 170. | 171. | 172. | 173. | 174. | 175. | 176. | 177. | 178. | 179. | 180. | 181. | 182. | 183. | 184. | 185. | 186. | 187. | 188. | 189. | 190. | 191. | 192. | 193. | 194. | 195. | 196. | 197. | 198. | 199. | 200. | 201. | 202. | 203. | 205. | 206. | 207. | 208. | 209. | 210. | 211. | 212. | 213. | 214. | 215. | 216. | 217. | 218. | 219. | 220. | 221. | 222. | 223. | 224. | 225. | 226. | 227. | 228. | 229. | 230. | 231. | 232. | 233. | 234. | 235. | 236. | 237. | 238. | 239. | 240. | 332. | 241. | 242. | 243. | 244. | 245. | 246. | 247. | 248. | 249. | 250. | 251. | 252. | 253. | 254. | 255. | 257. | 258. | 259. | 261. | 262. | 263. | 264. | 265. | 266. | 267. | 268. | 269. | 270. | 271. | 272. | 273. | 274. | 276. | 277. | 278. | 279. | 280. | 281. | 282. | 283. | 284. | 285. | 286. | 287. | 288. | 289. | 290. | 291. | 292. | 293. | 294. | 297. | 298. | 299. | 300. | 301. | 302. | 303. | 304. | 306. | 307. | 308. | 309. | 310. | 311. | 312. | 313. | 314. | 315. | 316. | 317. | 318. | 319. | 320. | 321. | 322. | 323. | 324. | 325. | 326. | 327. | 328. | 329. | 330. | 331. | 332. | 333. | 334. | 335. | 336. | 339. | 340. | 341. | 338. | 342. | 343. | 345. | 346. | 347. | 348. | 349. | 350. | 351. | 352. | 353. | 354. | 355. | 356. | 357. | 358. | 2. | "
     ]
    }
   ],
   "source": [
    "# Find section numbers like \" 1.\" not followed by \"-\"\n",
    "# Pattern: space + number + period, NOT followed by dash\n",
    "pattern = r'\\s+(\\d{1,3})\\.(?!\\s*[-–—])'\n",
    "\n",
    "sections = []\n",
    "for match in re.finditer(pattern, text):\n",
    "    section_num = match.group(1)\n",
    "    sections.append(int(section_num))\n",
    "\n",
    "    cleaned = match.group(0).strip()\n",
    "    if cleaned:\n",
    "        print(cleaned,end=\" | \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d618a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a934abf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102]\n"
     ]
    }
   ],
   "source": [
    "pattern = r'--- Page (\\d+) ---'\n",
    "\n",
    "page_numbers = [int(n) for n in re.findall(pattern, text)]\n",
    "\n",
    "print(page_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401d22f",
   "metadata": {},
   "source": [
    "#### for sub sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bc7da4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 8 ) ( 9 ) ( 10 ) ( 11 ) ( 12 ) ( 13 ) ( 14 ) ( 15 ) ( 16 ) ( 17 ) ( 18 ) ( 19 ) ( 20 ) ( 21 ) ( 22 ) ( 23 ) ( 24 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 7 ) ( 8 ) ( 6 ) ( 7 ) ( 2 ) ( 1 ) ( 1 ) ( 25 ) ( 26 ) ( 27 ) ( 28 ) ( 31 ) ( 45 ) ( 29 ) ( 30 ) ( 31 ) ( 32 ) ( 33 ) ( 34 ) ( 35 ) ( 36 ) ( 37 ) ( 38 ) ( 39 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 8 ) ( 9 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 1 ) ( 2 ) ( 1 ) ( 3 ) ( 1 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 1 ) ( 2 ) ( 1 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 1 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 1 ) ( 2 ) ( 1 ) ( 1 ) ( 2 ) ( 2 ) ( 3 ) ( 1 ) ( 4 ) ( 1 ) ( 1 ) ( 2 ) ( 2 ) ( 1 ) ( 1 ) ( 2 ) ( 1 ) ( 1 ) ( 2 ) ( 1 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 8 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 3 ) ( 1 ) ( 2 ) ( 1 ) ( 1 ) ( 3 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 1 ) ( 2 ) ( 1 ) ( 3 ) ( 1 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 12 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 1 ) ( 3 ) ( 6 ) ( 7 ) ( 8 ) ( 1 ) ( 9 ) ( 8 ) ( 1 ) ( 2 ) ( 3 ) ( 1 ) ( 2 ) ( 3 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 1 ) ( 2 ) ( 1 ) ( 1 ) ( 1 ) ( 4 ) ( 2 ) ( 3 ) ( 4 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 4 ) ( 6 ) ( 7 ) ( 8 ) ( 1 ) ( 2 ) ( 1 ) ( 1 ) ( 2 ) ( 1 ) ( 1 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 4 ) ( 6 ) ( 7 ) ( 8 ) ( 2 ) ( 3 ) ( 4 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 4 ) ( 6 ) ( 7 ) ( 8 ) ( 1 ) ( 2 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 1 ) ( 2 ) ( 1 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 5 ) ( 6 ) ( 7 ) ( 8 ) ( 1 ) ( 2 ) ( 1 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 1 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 3 ) ( 1 ) ( 2 ) ( 1 ) ( 2 ) ( 1 ) ( 1 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 1 ) ( 1 ) ( 2 ) ( 3 ) ( 2 ) ( 1 ) ( 2 ) ( 3 ) ( 4 ) ( 1 ) ( 2 ) ( 1 ) ( 3 ) ( 4 ) ( 2 ) "
     ]
    }
   ],
   "source": [
    "# Find numbers in format \"( 3 )\" - parentheses with spaces around number\n",
    "pattern = r'\\(\\s+(\\d+)\\s+\\)'\n",
    "\n",
    "numbered_items = []\n",
    "for match in re.finditer(pattern, text):\n",
    "    num = match.group(1)\n",
    "    numbered_items.append(int(num))\n",
    "    print(f\"{match.group(0)}\", end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6effd224",
   "metadata": {},
   "source": [
    "### Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b976a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 532\n",
      "Saved to chunks.json\n",
      "\n",
      "Sample: Page 1 | CHAPTER I | Section 1 | Subsection: (1)\n",
      "Content: ThisAct maybe called the Bharatiya Nyaya Sanhita, 2023...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "pdf_name = \"THE_BHARATIYA_NYAYA_SANHITA_2023.pdf\"\n",
    "source_url = \"https://www.mha.gov.in/sites/default/files/250883_english_01042024.pdf\"\n",
    "\n",
    "with open('extracted_text.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Clean heading helper\n",
    "def clean_heading(h):\n",
    "    parts = h.strip().lstrip().split()\n",
    "    cleaned = []\n",
    "    i = 0\n",
    "    while i < len(parts):\n",
    "        if len(parts[i]) == 1 and i + 1 < len(parts) and parts[i+1][0].isupper():\n",
    "            cleaned.append(parts[i] + parts[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            cleaned.append(parts[i])\n",
    "            i += 1\n",
    "    return ' '.join(cleaned).replace(' ,', ',')\n",
    "\n",
    "# Extract all markers with positions\n",
    "page_positions = [(int(m.group(1)), m.start()) for m in re.finditer(r'--- Page (\\d+) ---', text)]\n",
    "chapters = [(m.group(1), clean_heading(m.group(2)), m.start()) for m in re.finditer(r'CHAPTER([IVX]+)\\s+([A-Z\\s,]+?)(?=\\s+\\d+\\.|$)', text)]\n",
    "sections = [(int(m.group(1)), m.start()) for m in re.finditer(r'\\s+(\\d{1,3})\\.(?!\\s*[-–—])', text)]\n",
    "# Subsection pattern: full stop + space + ( number ) - avoids picking numbers from middle of sentences\n",
    "subsections = [(int(m.group(1)), m.start()) for m in re.finditer(r'\\.\\s+\\(\\s+(\\d+)\\s+\\)', text)]\n",
    "\n",
    "# Helper to get page number for a position\n",
    "def get_page(pos):\n",
    "    for i, (pnum, ppos) in enumerate(page_positions):\n",
    "        if i + 1 < len(page_positions):\n",
    "            if ppos <= pos < page_positions[i+1][1]:\n",
    "                return pnum\n",
    "        elif ppos <= pos:\n",
    "            return pnum\n",
    "    return None\n",
    "\n",
    "# Helper to find subsections within a section\n",
    "def get_subsections_for_section(sec_pos, sec_end_pos):\n",
    "    return [(num, pos) for num, pos in subsections if sec_pos <= pos < sec_end_pos]\n",
    "\n",
    "# Create chunks hierarchically: Chapter -> Section -> Subsection\n",
    "chunks = []\n",
    "\n",
    "# Iterate through chapters\n",
    "for i, (chap_num, chap_heading, chap_pos) in enumerate(chapters):\n",
    "    # Get sections for this chapter\n",
    "    chap_end_pos = chapters[i+1][2] if i+1 < len(chapters) else len(text)\n",
    "    chapter_sections = [(num, pos) for num, pos in sections if chap_pos <= pos < chap_end_pos]\n",
    "    \n",
    "    # Iterate through sections in this chapter\n",
    "    for j, (sec_num, sec_pos) in enumerate(chapter_sections):\n",
    "        # Get subsections for this section\n",
    "        sec_end_pos = chapter_sections[j+1][1] if j+1 < len(chapter_sections) else chap_end_pos\n",
    "        section_subsections = get_subsections_for_section(sec_pos, sec_end_pos)\n",
    "        \n",
    "        if section_subsections:\n",
    "            # Section has subsections - create chunk for each\n",
    "            for k, (subnum, subpos) in enumerate(section_subsections):\n",
    "                sub_end_pos = section_subsections[k+1][1] if k+1 < len(section_subsections) else sec_end_pos\n",
    "                # subpos points to period, find actual ( number ) position for page tracking\n",
    "                subsection_match = re.search(r'\\(\\s+\\d+\\s+\\)', text[subpos:subpos+20])\n",
    "                actual_subpos = subpos + subsection_match.start() if subsection_match else subpos\n",
    "                # Remove period + space + ( number ) from content\n",
    "                content = re.sub(r'\\.\\s+\\(\\s+\\d+\\s+\\)', '', text[subpos:sub_end_pos], count=1).strip()\n",
    "                \n",
    "                page_num = get_page(actual_subpos)\n",
    "                chunks.append({\n",
    "                    'page_number': page_num,\n",
    "                    'pdf_name': pdf_name,\n",
    "                    'source_url': f'{source_url}#page={page_num}' if page_num else source_url,\n",
    "                    'chapter': f'CHAPTER {chap_num}',\n",
    "                    'chapter_heading': chap_heading,\n",
    "                    'section': f'Section {sec_num}',\n",
    "                    'subsection': f'({subnum})',\n",
    "                    'content': content\n",
    "                })\n",
    "        else:\n",
    "            # Section has no subsections - create single chunk for entire section\n",
    "            content = text[sec_pos:sec_end_pos].strip()\n",
    "            # Remove section number pattern if at start\n",
    "            content = re.sub(r'^\\s*\\d+\\.\\s*', '', content).strip()\n",
    "            \n",
    "            page_num = get_page(sec_pos)\n",
    "            chunks.append({\n",
    "                'page_number': page_num,\n",
    "                'pdf_name': pdf_name,\n",
    "                'source_url': f'{source_url}#page={page_num}' if page_num else source_url,\n",
    "                'chapter': f'CHAPTER {chap_num}',\n",
    "                'chapter_heading': chap_heading,\n",
    "                'section': f'Section {sec_num}',\n",
    "                'subsection': None,\n",
    "                'content': content\n",
    "            })\n",
    "\n",
    "# Save chunks to JSON\n",
    "with open('chunks.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(f\"Saved to chunks.json\")\n",
    "if chunks:\n",
    "    c = chunks[0]\n",
    "    print(f\"\\nSample: Page {c['page_number']} | {c['chapter']} | {c['section']} | Subsection: {c['subsection']}\")\n",
    "    print(f\"Content: {c['content'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb08d6e",
   "metadata": {},
   "source": [
    "### FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9f50b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import List, Dict\n",
    "import os\n",
    "\n",
    "class LegalFAISSIndex:\n",
    "    def __init__(self, openai_api_key: str):\n",
    "        \"\"\"\n",
    "        Initialize FAISS for legal document retrieval using OpenAI embeddings (text-embedding-3-large)\n",
    "        \"\"\"\n",
    "        self.embedding_model = OpenAIEmbeddings(\n",
    "            model='text-embedding-3-large',\n",
    "            api_key=openai_api_key,\n",
    "        )\n",
    "        # OpenAI text-embedding-3-large has 3072 dimensions\n",
    "        self.dimension = 3072\n",
    "        self.index = None\n",
    "        self.chunks = []\n",
    "        \n",
    "    def create_index(self, chunks: List[Dict]):\n",
    "        \"\"\"\n",
    "        Create FAISS Flat index for maximum accuracy\n",
    "        \"\"\"\n",
    "        self.chunks = chunks\n",
    "        texts = [chunk['content'] for chunk in chunks]\n",
    "        \n",
    "        # Generate embeddings using OpenAI\n",
    "        print(\"Generating embeddings with OpenAI text-embedding-3-large...\")\n",
    "        embeddings = self.embedding_model.embed_documents(texts)\n",
    "        embeddings = np.array(embeddings).astype('float32')\n",
    "        \n",
    "        # Normalize for cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        \n",
    "        # Use Flat index for maximum accuracy\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)\n",
    "        self.index.add(embeddings)\n",
    "        print(f\"Added {len(embeddings)} vectors to Flat index\")\n",
    "        \n",
    "    def search(self, query: str, k: int = 5):\n",
    "        \"\"\"\n",
    "        Search for relevant sections using pure semantic search\n",
    "        \n",
    "        Args:\n",
    "            query: Legal query or crime description\n",
    "            k: Number of results to return\n",
    "        \"\"\"\n",
    "        # Generate query embedding\n",
    "        query_vector = self.embedding_model.embed_query(query)\n",
    "        query_vector = np.array([query_vector]).astype('float32')\n",
    "        faiss.normalize_L2(query_vector)\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_vector, k)\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in zip(indices[0], scores[0]):\n",
    "            if idx < len(self.chunks):\n",
    "                result = {\n",
    "                    'chunk': self.chunks[idx],\n",
    "                    'score': float(score)\n",
    "                }\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_index(self, index_path: str, chunks_path: str):\n",
    "        \"\"\"Save FAISS index and chunks\"\"\"\n",
    "        faiss.write_index(self.index, index_path)\n",
    "        import json\n",
    "        with open(chunks_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.chunks, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Saved index to {index_path} and chunks to {chunks_path}\")\n",
    "    \n",
    "    def load_index(self, index_path: str, chunks_path: str):\n",
    "        \"\"\"Load FAISS index and chunks\"\"\"\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        import json\n",
    "        with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "            self.chunks = json.load(f)\n",
    "        print(f\"Loaded index from {index_path} and {len(self.chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcdb257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings with OpenAI text-embedding-3-large...\n",
      "Added 532 vectors to Flat index\n",
      "Saved index to legal_index.faiss and chunks to chunks.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load chunks\n",
    "with open('chunks.json', 'r', encoding='utf-8') as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "# Create index\n",
    "indexer = LegalFAISSIndex(openai_api_key=OPENAI_API_KEY)\n",
    "indexer.create_index(chunks)\n",
    "\n",
    "# Optional: Save index for future use\n",
    "indexer.save_index('legal_index.faiss', 'chunks.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb4b4fa",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6be71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the punishment for murder?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[1] Score: 0.5848\n",
      "    CHAPTER III - GENERAL EXCEPTIONS\n",
      "    Section 103 - (1)\n",
      "    Page: 34\n",
      "    Content: Whoever commits murder shall be punished with death or imprisonment for life, and shall also be liable to fine...\n",
      "    Source: https://www.mha.gov.in/sites/default/files/250883_english_01042024.pdf#page=34\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] Score: 0.5826\n",
      "    CHAPTER III - GENERAL EXCEPTIONS\n",
      "    Section 104 - None\n",
      "    Page: 34\n",
      "    Content: Whoever, being under sentence of imprisonment for life, commits murder, shall be punished with death or with imprisonment for life, which shall mean the remainder of that person’s natural life....\n",
      "    Source: https://www.mha.gov.in/sites/default/files/250883_english_01042024.pdf#page=34\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] Score: 0.5433\n",
      "    CHAPTER III - GENERAL EXCEPTIONS\n",
      "    Section 105 - None\n",
      "    Page: 34\n",
      "    Content: Whoever commits culpable homicide not amounting to murder, shall be punished with imprisonment for life, or imprisonment of either description for a term which shall not be less than five years but wh...\n",
      "    Source: https://www.mha.gov.in/sites/default/files/250883_english_01042024.pdf#page=34\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[4] Score: 0.4814\n",
      "    CHAPTER III - GENERAL EXCEPTIONS\n",
      "    Section 109 - (1)\n",
      "    Page: 34\n",
      "    Content: Whoever does any act with such intention or knowledge, and under such circumstances that, if he by that act caused death, he would be guilty of murder, shall be punished with imprisonment of either de...\n",
      "    Source: https://www.mha.gov.in/sites/default/files/250883_english_01042024.pdf#page=34\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[5] Score: 0.4764\n",
      "    CHAPTER III - GENERAL EXCEPTIONS\n",
      "    Section 103 - (2)\n",
      "    Page: 34\n",
      "    Content: When a group of five or more persons acting in concert commits murder on the ground of race, caste or community, sex, place of birth, language, personal belief or any other similar ground each member ...\n",
      "    Source: https://www.mha.gov.in/sites/default/files/250883_english_01042024.pdf#page=34\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load index and chunks\n",
    "index = faiss.read_index('legal_index.faiss')\n",
    "with open('chunks.json', 'r', encoding='utf-8') as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = OpenAIEmbeddings(model='text-embedding-3-large', api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Query\n",
    "query = \"What is the punishment for murder?\"\n",
    "k = 5\n",
    "\n",
    "# Generate query embedding\n",
    "query_vector = embedding_model.embed_query(query)\n",
    "query_vector = np.array([query_vector]).astype('float32')\n",
    "faiss.normalize_L2(query_vector)\n",
    "\n",
    "# Search\n",
    "scores, indices = index.search(query_vector, k)\n",
    "\n",
    "# Display results\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "for i, (idx, score) in enumerate(zip(indices[0], scores[0]), 1):\n",
    "    chunk = chunks[idx]\n",
    "    print(f\"\\n[{i}] Score: {score:.4f}\")\n",
    "    print(f\"    {chunk['chapter']} - {chunk['chapter_heading']}\")\n",
    "    print(f\"    {chunk['section']} - {chunk.get('subsection', 'N/A')}\")\n",
    "    print(f\"    Page: {chunk['page_number']}\")\n",
    "    print(f\"    Content: {chunk['content'][:200]}...\")\n",
    "    print(f\"    Source: {chunk['source_url']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
